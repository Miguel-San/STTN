{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from matplotlib.pyplot import imshow\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "from torchvision import models\n",
    "import torch.multiprocessing as mp\n",
    "from torchvision import transforms\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "import importlib\n",
    "import os\n",
    "import argparse\n",
    "import copy\n",
    "import datetime\n",
    "import random\n",
    "import sys\n",
    "import json\n",
    "\n",
    "### My libs\n",
    "from core.utils import Stack, ToTorchFormatTensor\n",
    "from core.utils import ZipReader\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_length = 5\n",
    "neighbor_stride = 1\n",
    "\n",
    "save_dir = \"vis_tests/\"\n",
    "os.makedirs(save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.set_device(2)\n",
    "device = 'cuda:2'\n",
    "    \n",
    "CKPT = '/home/m.sanchez/Documents/STTN/release_model_dev/sttn_twin_jet_old_scaled_dev/gen_04500.pth'\n",
    "MODEL_NAME = 'vis'\n",
    "# MODEL_NAME = 'sttn'\n",
    "# w, h = 432, 240\n",
    "w, h = 512, 352\n",
    "\n",
    "args = {\n",
    "    'data_root': 'datasets/',\n",
    "    'name': 'twin_jet_old_scaled',\n",
    "    'size': (w, h),\n",
    "    'split' : 'test'\n",
    "}\n",
    "\n",
    "_to_tensors = transforms.Compose([\n",
    "  Stack(),\n",
    "  ToTorchFormatTensor()])\n",
    "\n",
    "\n",
    "# def get_ref_index(neighbor_ids, length, split):\n",
    "#   ref_index = []\n",
    "#   for i in range(0, length, ref_length):\n",
    "#     if not i in neighbor_ids:\n",
    "#       ref_index.append(i)\n",
    "#   return ref_index\n",
    "\n",
    "\n",
    "# def get_mask(vname, f):\n",
    "#   if MASK_TYPE == 'fixed':\n",
    "#     m = np.zeros((h, w), np.uint8)\n",
    "#     m[h//2-h//8:h//2+h//8, w//2-w//8:w//2+w//8] = 255\n",
    "#     return Image.fromarray(m)\n",
    "#   elif MASK_TYPE == 'object':\n",
    "#     mname = f\"{str(f).zfill(5)}.png\"\n",
    "#     m = ZipReader.imread('datasets/{}/Annotations/{}.zip'.format(DATA_NAME, vname), mname).convert('L')\n",
    "#     m = np.array(m)\n",
    "#     m = np.array(m>0).astype(np.uint8)\n",
    "#     m = cv2.resize(m, (w,h), cv2.INTER_NEAREST)\n",
    "#     m = cv2.dilate(m, cv2.getStructuringElement(cv2.MORPH_CROSS,(3,3)), iterations=4)\n",
    "#     return Image.fromarray(m*255)\n",
    "#   elif MASK_TYPE == 'random_obj':\n",
    "#     m = ZipReader.imread('datasets/random_masks/{}.zip'.format(DATA_NAME),\\\n",
    "#       '{}.png'.format(vname)).resize((w, h))\n",
    "#     m = np.array(m)\n",
    "#     m = np.array(m>0).astype(np.uint8)\n",
    "#     return Image.fromarray(m*255)\n",
    "#   else:\n",
    "#     raise NotImplementedError(f\"Mask type {MASK_TYPE} not exists\")\n",
    "\n",
    "def get_ref_index(length, sample_length, stride, pivot=None):\n",
    "    if pivot is None:\n",
    "        pivot = random.randint(0, length-sample_length-(stride-1)*(sample_length-1))\n",
    "    ref_index = [pivot+i*stride for i in range(sample_length)]\n",
    "    return ref_index\n",
    "\n",
    "def get_mask(vname, f):\n",
    "  all_masks = []\n",
    "  for fname in os.listdir('{}/{}/masks'.format(args['data_root'], args['name'])):\n",
    "      mask = Image.fromarray(cv2.imread('{}/{}/masks'.format(\n",
    "          args['data_root'], args['name']) + '/' + fname)).convert(\"L\")\n",
    "      mask = mask.resize(args[\"size\"])\n",
    "      all_masks.append(mask)\n",
    "\n",
    "  return all_masks[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save ann to img by pyplt\n",
    "def save_by_pyplt(I, anns, fname, cmap, vxtrm=None):\n",
    "    dpi=100\n",
    "    shape=np.shape(I)[0:2][::-1]\n",
    "    size = [float(i)/dpi for i in shape]\n",
    "    fig = plt.figure()\n",
    "    fig.set_size_inches(size)\n",
    "    ax = plt.Axes(fig,[0,0,1,1])\n",
    "    ax.set_axis_off()\n",
    "    fig.add_axes(ax)\n",
    "    ax.imshow(I, cmap=\"gray\")\n",
    "    if anns is not None:\n",
    "      if vxtrm is not None:\n",
    "        vmin=0\n",
    "        vmax=vxtrm\n",
    "      else:\n",
    "        vmin=None\n",
    "        vmax=None\n",
    "      ax.imshow(anns, alpha=0.7, cmap=cmap, vmin=vmin, vmax=vmax)\n",
    "    fig.savefig(fname, dpi=dpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and version\n",
    "net = importlib.import_module('model.' + MODEL_NAME)\n",
    "model = net.InpaintGenerator().to(device)\n",
    "data = torch.load(CKPT, map_location = device) \n",
    "model.load_state_dict(data['netG'])\n",
    "print('loading from: {}'.format(CKPT))\n",
    "model.eval()\n",
    "\n",
    "# prepare dataset\n",
    "save_path = \"./\"\n",
    "with open(os.path.join(args['data_root'], args['name'], args[\"split\"]+'.json'), 'r') as f:\n",
    "    video_dict = json.load(f)\n",
    "    video_names = list(video_dict.keys())\n",
    "video_names.sort()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vi = np.random.randint(0, len(video_names))\n",
    "# vname = video_names[vi]\n",
    "# print(vi)\n",
    "\n",
    "# frame_names = [f\"{str(i).zfill(5)}.png\" for i in range(video_dict[vname])]\n",
    "# num_frames = video_dict[vname]\n",
    "# print('{} of {} frames ...'.format(vname, num_frames))\n",
    "# masks = []\n",
    "# frames = []\n",
    "# orig_frames = []\n",
    "\n",
    "# # preprocess data\n",
    "# for f, fname in enumerate(frame_names):\n",
    "#   # img = ZipReader.imread('datasets/{}/JPEGImages/{}.zip'.format(args[\"data_root\"], args[\"name\"], vname), fname).convert('RGB')\n",
    "#   img = Image.fromarray(cv2.imread('{}/{}/JPEGImages/{}'.format(\n",
    "#                 args['data_root'], args['name'], vname) + '/' + fname)).convert(\"L\")\n",
    "#   orig_frames.append(img)\n",
    "#   frames.append(img.resize((w, h)))\n",
    "#   m = get_mask(vname, f)\n",
    "#   masks.append(m)\n",
    "\n",
    "# # binary_masks = [np.expand_dims((np.array(i)!=0).astype(np.uint8), 2) for i in masks]\n",
    "# binary_masks = [np.expand_dims((np.array(i)!=0), 2) for i in masks]\n",
    "# comp_frames = [None]*len(frame_names)\n",
    "# pred_frames = [None]*len(frame_names)\n",
    "\n",
    "# feats = _to_tensors(frames).unsqueeze(0)*2.0 - 1.0\n",
    "# # frames = [np.array(i).astype(np.uint8) for i in frames]\n",
    "# frames = [np.array(i) for i in frames]\n",
    "# masks =  _to_tensors(masks).unsqueeze(0)\n",
    "# feats, masks = feats.to(device), masks.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "#   feats = model.encoder((feats*(1.0-masks).float()).view(num_frames, 1, h, w))\n",
    "#   _, c, feat_h, feat_w = feats.size()\n",
    "#   feats = feats.view(1, num_frames, c, feat_h, feat_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # begin inference \n",
    "# for f in range(len(frame_names)//2, len(frame_names), neighbor_stride):\n",
    "#   neighbor_ids = [i for i in range(max(0,f-neighbor_stride), min(len(frame_names), f+neighbor_stride+1))]\n",
    "#   # ref_ids = get_ref_index(neighbor_ids, len(frame_names), ref_length)\n",
    "#   ref_ids = get_ref_index(len(frame_names), ref_length, 1, f)\n",
    "#   with torch.no_grad():\n",
    "#     current_feat, attn, mm = model.infer(feats[0,neighbor_ids+ref_ids, :,:,:], masks[0,neighbor_ids+ref_ids,:,:,:])\n",
    "#     current_img = torch.tanh(model.decoder(current_feat[:len(neighbor_ids),:,:,:])).detach()\n",
    "# #     current_img = torch.tanh(model.decoder(current_feat)).detach()\n",
    "#     pred_img = (current_img+1)/2\n",
    "#     pred_img = pred_img.cpu().permute(0,2,3,1).numpy()*255\n",
    "#     # visualize attention \n",
    "#     vis_img = np.array(pred_img[0]).astype(np.uint8)\n",
    "#     imshow(vis_img)\n",
    "    \n",
    "#   print('show...')\n",
    "#   break\n",
    "#   input('Enter something...')\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# print('neighbor_ids: ', neighbor_ids, len(neighbor_ids))\n",
    "# print('ref ids: ', ref_ids, len(ref_ids))\n",
    "# ids = neighbor_ids+ref_ids\n",
    "# print('ids: ', ids, len(ids))\n",
    "\n",
    "# img = np.array(pred_img[len(neighbor_ids)//2+1]).astype(np.uint8)\n",
    "# input = frames[f]*(1-np.squeeze(binary_masks[f]))+255*np.squeeze(binary_masks[f])\n",
    "# a = mm[len(neighbor_ids)//2+1].cpu().numpy().astype(np.float32)\n",
    "# print('selected attention patch size: ', mm.size()[0])\n",
    "\n",
    "# # select a target patch \n",
    "# start = np.argmax(a)\n",
    "# end  = a.size - np.argmax(a[::-1]) - 1\n",
    "# a = cv2.resize(a, (w, h))\n",
    "# selected_patch=(start+end)//2\n",
    "\n",
    "# print(start, end, selected_patch)\n",
    "# print('target frame as {}'.format(f))\n",
    "# save_by_pyplt(frames[f], None, save_dir+'groundtruth_{}.jpg'.format(str(f).zfill(3)), 'gray')\n",
    "# save_by_pyplt(input, None, save_dir+'input_{}.jpg'.format(str(f).zfill(3)), 'gray')\n",
    "# save_by_pyplt(img, None, save_dir+'output_{}.jpg'.format(str(f).zfill(3)), 'gray')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# save_by_pyplt(input, None, save_dir+'target.jpg', 'gray')\n",
    "# plt.show()\n",
    "\n",
    "# # find all attention value for selected patch \n",
    "# np_attn = attn.cpu().numpy()\n",
    "# val = np_attn[selected_patch]\n",
    "\n",
    "# # sort attention \n",
    "# ind = np.unravel_index(np.argsort(val, axis=None), val.shape)\n",
    "# ind_t, ind_x, ind_y = list(ind[0]), list(ind[1]), list(ind[2])\n",
    "# ind_t.reverse()\n",
    "# seen = set()\n",
    "# ind_t = np.array([x for x in ind_t if x not in seen and not seen.add(x)])\n",
    "# print(ind_t)\n",
    "\n",
    "# # show top 5 attended frames\n",
    "# for t in ind_t[:5]:\n",
    "#   print('Attention from frame: {}'.format(ids[t]))\n",
    "#   ref = frames[ids[t]]*(1-np.squeeze(binary_masks[ids[t]]))+255*np.squeeze(binary_masks[ids[t]])\n",
    "#   a = cv2.resize(val[t], (w, h))\n",
    "#   save_by_pyplt(ref, a, save_dir+'ref_{}.jpg'.format(str(ids[t]).zfill(3)), None)\n",
    "#   plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_frame = 75\n",
    "win_len = 5\n",
    "stride=1\n",
    "\n",
    "chosen_frames = list(range(target_frame-win_len//2, target_frame+win_len//2+1, stride))\n",
    "num_frames = len(chosen_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vi = np.random.randint(0, len(video_names))\n",
    "vname = video_names[vi]\n",
    "\n",
    "frame_names = [f\"{str(i).zfill(5)}.png\" for i in range(video_dict[vname])]\n",
    "print('{} of {} frames ...'.format(vname, video_dict[vname]))\n",
    "masks = []\n",
    "frames = []\n",
    "orig_frames = []\n",
    "\n",
    "# preprocess data\n",
    "for f, fname in enumerate([frame_names[i] for i in chosen_frames]):\n",
    "  # img = ZipReader.imread('datasets/{}/JPEGImages/{}.zip'.format(args[\"data_root\"], args[\"name\"], vname), fname).convert('RGB')\n",
    "  img = Image.fromarray(cv2.imread('{}/{}/JPEGImages/{}'.format(\n",
    "                args['data_root'], args['name'], vname) + '/' + fname)).convert(\"L\")\n",
    "  orig_frames.append(img)\n",
    "  frames.append(img.resize((w, h)))\n",
    "  m = get_mask(vname, f)\n",
    "  masks.append(m)\n",
    "\n",
    "# binary_masks = [np.expand_dims((np.array(i)!=0).astype(np.uint8), 2) for i in masks]\n",
    "binary_masks = [np.expand_dims((np.array(i)!=0), 2) for i in masks]\n",
    "comp_frames = [None]*len(frame_names)\n",
    "pred_frames = [None]*len(frame_names)\n",
    "\n",
    "frame_tensors = _to_tensors(frames).unsqueeze(0)*2.0 - 1.0\n",
    "# frames = [np.array(i).astype(np.uint8) for i in frames]\n",
    "frames = [np.array(i) for i in frames]\n",
    "masks =  _to_tensors(masks).unsqueeze(0)\n",
    "frame_tensors, masks = frame_tensors.to(device), masks.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "  input_frames = frame_tensors*(1.0-masks).float()\n",
    "  feats = model.encoder(input_frames.view(num_frames, 1, h, w))\n",
    "  _, c, feat_h, feat_w = feats.size()\n",
    "  feats = feats.view(1, num_frames, c, feat_h, feat_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference\n",
    "with torch.no_grad():\n",
    "  current_feat, attn, mm = model.infer(feats[0], masks[0])\n",
    "  current_img = torch.tanh(model.decoder(current_feat)).detach()\n",
    "  pred_img = (current_img+1.)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, f in enumerate(chosen_frames):\n",
    "    save_by_pyplt(np.squeeze(frame_tensors.cpu().numpy())[i], None, save_dir+'groundtruth_{}.jpg'.format(str(f).zfill(3)), 'gray')\n",
    "    save_by_pyplt(np.squeeze(input_frames.cpu().numpy())[i], None, save_dir+'input_{}.jpg'.format(str(f).zfill(3)), 'gray')\n",
    "    save_by_pyplt(np.squeeze(pred_img.cpu().numpy())[i], None, save_dir+'output_{}.jpg'.format(str(f).zfill(3)), 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_attn = attn.cpu().numpy()\n",
    "print(np_attn.shape)\n",
    "n_patches = np_attn.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_results_path = save_dir+'attn_results/'\n",
    "os.makedirs(attn_results_path, exist_ok=True)\n",
    "\n",
    "vxtrm = np.max(np.abs(np_attn))\n",
    "\n",
    "for i, f in enumerate(chosen_frames):\n",
    "    for p in range(n_patches):\n",
    "        attn_val = np_attn[p,i,...]\n",
    "        attn_val_rsh = cv2.resize(attn_val, (w, h))\n",
    "\n",
    "        save_by_pyplt(np.squeeze(input_frames.cpu().numpy())[i], attn_val_rsh, attn_results_path+'attn_from_f{}_p{}.jpg'.format(f, p).zfill(3), None, vxtrm=vxtrm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIRML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
